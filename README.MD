## **Overview**

This system allows users to upload PDF documents, index their content, and interact with the indexed documents using a conversational interface. The system uses Pinecone for vector storage, OpenAI's GPT model for querying knowledge, and ChromaDB for document indexing.

## **File: pdf_loader.py**

### **Purpose**

The pdf_loader.py module is responsible for loading and processing PDF documents, converting them into a format that can be indexed by a vector database (ChromaDB). It extracts text from the PDF files and stores it in a way that can later be used for similarity searches.

### **Key Functions**

#### **load_pdfs_to_chroma(upload_directory: str)**

Loads the uploaded PDFs into ChromaDB for further processing and indexing.

**Parameters**:

* upload_directory (str): The directory containing the uploaded PDF files.

**Description**:

* This function reads the PDF files from the specified directory.
* It processes each file, extracts the text, and indexes it in ChromaDB using appropriate embeddings for efficient searching.

**Example Usage**:

```
load_pdfs_to_chroma("uploads")
```

## **File: rag_query.py**

### **Purpose**

The rag_query.py module handles querying the knowledge base using both document context and external knowledge (via OpenAI GPT). It integrates Pinecone for document search and OpenAI for generating responses to user queries.

### **Key Functions**

#### **query_knowledge_base(query: str, use_gpt_knowledge: bool = True) -> str**

Queries the knowledge base using the provided query and returns the response based on document content or GPT knowledge.

**Parameters**:

* query (str): The question or query the user wants to ask.
* use_gpt_knowledge (bool): Whether to include general GPT knowledge along with the document context. Defaults to True.

**Returns**:

* (str): The response generated by the system, either based on document context, GPT knowledge, or both.

**Description**:

* The function initializes the OpenAI and Pinecone components for document search and response generation.
* It performs a similarity search in Pinecone to find relevant documents and combines the context with GPT knowledge to generate a response.

**Example Usage**:

```
response = query_knowledge_base("What is the capital of France?", use_gpt_knowledge=True)
```

## **File: app.py**

### **Purpose**

The app.py module is the main Streamlit application for interacting with users. It sets up the user interface, allows file uploads, manages session state, and facilitates querying the knowledge base. Users can input questions, select topics, and upload documents to be indexed.

### **Key Components**

#### **Streamlit UI Elements**

* **File Uploader**: Allows users to upload multiple PDF files for processing and indexing.
* **Text Input**: Allows users to ask questions and provide a topic.
* **Chat Interface**: Displays the chat messages between the user and the assistant.
* **Sidebar**: Provides options to manage user profiles, toggle knowledge modes, and view upload progress.

#### **Session State Management**

Session state is used to manage the user's interactions with the system. It tracks the following:

* messages: Stores the chat history (user and assistant messages).
* setup_complete: Tracks whether document processing is complete.
* use_gpt_knowledge: Controls whether to use GPT knowledge in the responses.
* current_user: Stores the current logged-in user.
* current_topic: Stores the current topic of conversation.
* user_profile: Stores the user profile, which tracks interactions and progress.

#### **Key Functions**

##### **save_interaction(query, response)**

Saves the user's question and the assistant's response, updating the user profile with the context used (either GPT knowledge or document context).

**Parameters**:

* query (str): The user's question.
* response (str): The assistant's generated response.

**Description**:

* This function is called after each interaction to log the query, response, and context used in the session.

##### **Main App Flow:**

1. **User Profile Management**: Users can input their username and select a topic from the sidebar.
2. **Document Upload**: Users can upload multiple PDF files for processing.
3. **Querying the Knowledge Base**: Users can ask questions, and the system will generate responses based on the document context and GPT knowledge.
4. **Displaying the Results**: Responses are displayed in the chat interface, and user interactions are logged for progress tracking.

---

### **Example Workflow:**

1. The user uploads multiple PDF documents through the sidebar in app.py.
2. These PDFs are processed and indexed into ChromaDB via pdf_loader.py.
3. The user interacts with the system by entering queries. The system searches for relevant documents in Pinecone and generates a response using GPT knowledge if enabled.
4. The assistant's responses are displayed in the chat interface, and each interaction is logged using the save_interaction function.

---

## **Configuration and Environment**

### **Environment Variables**

The system uses the dotenv package to load environment variables from a .env file. The following environment variables should be set:

* PINECONE_API_KEY: Your Pinecone API key for accessing Pinecone services.
* PINECONE_ENVIRONMENT: The environment for your Pinecone instance (e.g., "us-west1-gcp").

---

### **Dependencies**

* streamlit: For the web application interface.
* pinecone: For vector storage and document similarity search.
* openai: For GPT-based response generation.
* chroma: For document indexing and retrieval.
* dotenv: For loading environment variables.
* pdfplumber: For extracting text from PDFs (implicitly required in pdf_loader.py).

---

### **Conclusion**

This documentation provides an overview of the three core components of the system: pdf_loader.py, rag_query.py, and app.py. Each component is responsible for handling a specific part of the overall workflow, from document uploading and indexing to querying the knowledge base and interacting with the user.
